{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入所需的库\n",
    "我们依旧会用gensim去做word2vec的处理，会用sklearn当中的SVM进行建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files (x86)\\python\\python35\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "d:\\program files (x86)\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 2015.12.09\n",
    "\n",
    "@author: Hanxiaoyang\n",
    "\"\"\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.externals import joblib#模型持久化模块，将模型保存至硬盘\n",
    "from sklearn.svm import SVC\n",
    "import sys  \n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "#不需要sys.setdefaultencoding(\"utf-8\")这段代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入数据，做预处理(分词)，切分训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg data:\n",
      "                                                   0\n",
      "0  做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...\n",
      "1  作者有明显的自恋倾向，只有有老公养不上班的太太们才能像她那样生活。很多方法都不实用，还有抄袭...\n",
      "2  作者完全是以一个过来的自认为是成功者的角度去写这个问题，感觉很不客观。虽然不是很喜欢，但是，...\n",
      "3       作者提倡内调，不信任化妆品，这点赞同。但是所列举的方法太麻烦，配料也不好找。不是太实用。\n",
      "4                  作者的文笔一般，观点也是和市面上的同类书大同小异，不推荐读者购买。\n",
      "pos data:\n",
      "                                                   0\n",
      "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...\n",
      "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...\n",
      "2  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...\n",
      "3  作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...\n",
      "4  作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...\n",
      "type of neg data= <class 'pandas.core.frame.DataFrame'>\n",
      "type of pos data= <class 'pandas.core.frame.DataFrame'>\n",
      "0        [做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...\n",
      "1        [作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...\n",
      "2        [作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, ...\n",
      "3        [作者, 在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, ...\n",
      "4        [作者, 在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数,...\n",
      "5        [作者, 有, 一种, 专业, 的, 谨慎, ，, 若能, 有幸, 学习, 原版, 也许, ...\n",
      "6        [作者, 用, 诗, 一样, 的, 语言, 把, 如水般, 清澈, 透明, 的, 思想, 娓...\n",
      "7        [作者, 提出, 了, 一种, 工作, 和, 生活, 的, 方式, ，, 作为, 咨询, 界...\n",
      "8        [作者, 妙语连珠, ，, 将, 整个, 60, -, 70, 年代, 用, 层出不穷, 的...\n",
      "9        [作者, 逻辑, 严密, ，, 一气呵成, 。, 没有, 一句, 废话, ，, 深入浅出, ...\n",
      "10       [作者, 力, 从, 马克思, 注意, 经济学, 角度, 来, 剖析, 当代, 中国, 经济...\n",
      "11       [作者, 结合, 希尔, 和, 卡耐基, 、, 汪中求, 等, 大师, 的, 一些, 观点,...\n",
      "12       [作者, 更, 多, 的, 是从, 圆圆, 母亲, 的, 角度, 来, 写, 这个, 文章,...\n",
      "13       [作者, 对于, 某些, 电影, “, 表面, 粗糙, ”, 、, “, 内里, 光滑, ”...\n",
      "14       [作者, 的, 理念, 很, 好, ，, 主要, 是, 看, 一个, 公司, 的, 内部, ...\n",
      "15       [作者, 的, 观点, 独特, ，, 语言, 犀利, ，, 深刻, 的, 总结, 了, 男人...\n",
      "16       [作者, 的, 笔触, 很, 真实,  , 将, 一个, 不忠, 却, 又, 深爱, 自己,...\n",
      "17       [作者, 笔下留情, 啊, ，, 深圳, 的, 自由, 作家, 远远, 没有, 《, 离婚,...\n",
      "18       [作者, 被, 认为, 是, “, 爱, 的, 奇迹, 天使, ”, ，, 确实, 是, 这...\n",
      "19       [作为, 有史以来, 最, 伟大, 的, 基金, 经理, 彼得, &#, 183, ;, 林...\n",
      "20       [作为, 一名, 山西, 太谷, 人, ，, 从小, 听, 多, 了, 有关, 晋商, 的,...\n",
      "21       [作为, 一名, “, 白骨精, ”, ，, 我, 不得不, 佩服, 作者, 入木三分, 的...\n",
      "22       [作为, 一本, 职场, 启示录, ，, 不错, 没有, 像, 杜, 拉拉, 一样, 的, ...\n",
      "23       [作为, 一本, 屹立, 不倒, 的, 经典, 教材, ，, 纽摄, 所, 传授, 的, 不...\n",
      "24       [作为, 一本, 西方人, 创作, 的, 心理学, 读物, ，, 还是, 以, 西方人, 的...\n",
      "25       [作为, 推理小说, 可能, 不是, 最好, 的, ，, 作为, 言情小说, 可能, 也, ...\n",
      "26       [作为, 女儿, 6.1, 的, 礼物, 。, 虽然, 晚到, 了, 几天, 。, 等, 拿...\n",
      "27       [作为, 妈妈, ，, 我, 个人, 很, 喜欢, 这, 本书, ，, 但是, 仍, 在, ...\n",
      "28       [昨晚, 看着, 看着, 就, 睡着, 了, ，, 今天, 早晨, 醒来, 就, 立马, 抓...\n",
      "29       [昨天, 我, 把, 这, 套书, 看, 完, 了, ，, 结尾, 我, 不是, 很, 喜欢...\n",
      "                               ...                        \n",
      "10647        [简单, ，, 安装, 很快, ，, 电话, 打, 了, 半小时, 就, 来, 装, 了]\n",
      "10648                           [挺, 好, ，, 卖家, 态度, 也, 很, 好]\n",
      "10649    [东西, 还, 不错,  ,  ,  ,  , 自己, 装, 起来, 了,  ,  ,  ,...\n",
      "10650          [很, 好, 用, ，, 快递, 很, 给, 力, ，, 帮, 我, 送到, 目的地]\n",
      "10651    [用过, 了, 才, 来, 评价, ，, 挺好用, 的, 和, 我, 在, 商场, 买, 的...\n",
      "10652    [17, 号, 付款, ，, 19, 号, 中午, 到货, ，, 下午, 售后, 就, 给,...\n",
      "10653    [第二次, 购买, 了, ，, 店家, 很, 热情, ，, 宝贝, 很, 完美, ，, 烧水...\n",
      "10654    [买来, 放在, 出租房, 里, 的, ，, 所以, 自己, 也, 没试, 过, ，, 但是...\n",
      "10655    [买来, 放在, 出租房, 里, 的, ，, 所以, 自己, 也, 没试, 过, ，, 但是...\n",
      "10656                               [东西, 不错, ，, 装上去, 用, 了]\n",
      "10657         [东西, 收到, 了, 很, 满意, ，, 快递, 也, 很快, 。, 。, 。, 。]\n",
      "10658    [超级, 好, 的, 卖家, ！, 之前, 不, 小心, 拍错, 了, ，, 客服, 非常,...\n",
      "10659    [还, 没, 拆货, ，, 物流, 给力, ，, 送货到, 家, ，, 一直, 信赖, 大,...\n",
      "10660                        [挺, 好, 的, ，, 卖家, 态度, 也, 很, 好]\n",
      "10661                            [挺, 好, 的, ，, 今天, 用, 了, 。]\n",
      "10662    [很, 好, 的, 热水器, ！, 很, 实际, 的, 功能, ！, 大, 品牌, ！,  ...\n",
      "10663                                    [好评, ！,  , 好评, ！]\n",
      "10664                                    [质量, 不错, ~, ~, ~]\n",
      "10665    [东西, 很, 好,  , 妈妈, 很, 喜欢,  , 大天, 客服,  , 服务, 很, ...\n",
      "10666    [发货, 很快, ，, 物流, 也, 及时, ，, 热水器, 包装, 很, 好, ，, 已经...\n",
      "10667                          [很, 不错, ，, 安装, 很, 到位, 哦, ！]\n",
      "10668    [东东, 不, 重, ，, 一个, 人, 就, 拉, 回家, 了, ，, 找人来, 安装, ...\n",
      "10669                         [安装, 好, 了,  , 挺, 好, 的, ！, 赞]\n",
      "10670    [宝贝, 很, 好, 很, 喜欢,  ,  , 卖家, 有, 多, 努力, 哦, ~, ~,...\n",
      "10671                                                 [好评]\n",
      "10672                                                 [好评]\n",
      "10673                                                 [好评]\n",
      "10674                                                 [好评]\n",
      "10675                                                 [好评]\n",
      "10676                                                 [好评]\n",
      "Name: words, Length: 10677, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#因为Excel需要单独的模块支持，所以需要安装xlrd模块\n",
    "#install xlrd\n",
    "neg=pd.read_excel('data/neg.xls',header=None,index=None)\n",
    "pos=pd.read_excel('data/pos.xls',header=None,index=None)#每行是一条评论文本\n",
    "print(\"neg data:\")\n",
    "print(neg.head(5))\n",
    "print(\"pos data:\")\n",
    "print(pos.head(5))\n",
    "\n",
    "print(\"type of neg data=\",type(neg))\n",
    "print(\"type of pos data=\",type(pos))\n",
    "\n",
    "#下面进行批量分词，较为耗时\n",
    "cw = lambda x: list(jieba.cut(x))#内部分词是用HMM\n",
    "#pos[0]返回columns为0的列，注意这种方式一次只能返回一个列。pos.0与pos['0']意思一样\n",
    "pos['words'] = pos[0].apply(cw)#对第一列，因为只有一列\n",
    "neg['words'] = neg[0].apply(cw)#在原来的对象基础上再增加一个words这一个新列\n",
    "print(pos['words'])#查看正样本的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0  \\\n",
      "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...   \n",
      "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...   \n",
      "\n",
      "                                               words  \n",
      "0  [做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...  \n",
      "1  [作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...  \n"
     ]
    }
   ],
   "source": [
    "print(type(pos))\n",
    "print(pos.head(2))#返回前2行的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  \\\n",
      "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...   \n",
      "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...   \n",
      "2  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...   \n",
      "\n",
      "                                               words  \n",
      "0  [做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...  \n",
      "1  [作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...  \n",
      "2  [作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, ...  \n"
     ]
    }
   ],
   "source": [
    "print(pos[0:3])#返回前3行的数据，列名分别是0，words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10677,)\n"
     ]
    }
   ],
   "source": [
    "print(np.ones(len(pos)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将上述的操作集成一个函数\n",
    "def load_file_and_preprocessing():\n",
    "    neg=pd.read_excel('data/neg.xls',header=None,index=None)\n",
    "    pos=pd.read_excel('data/pos.xls',header=None,index=None)\n",
    "    \n",
    "    cw = lambda x: list(jieba.cut(x))\n",
    "    pos['words'] = pos[0].apply(cw)\n",
    "    neg['words'] = neg[0].apply(cw)\n",
    "\n",
    "    #print pos['words']\n",
    "    #use 1 for positive sentiment, 0 for negative\n",
    "    y = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))#默认是沿着y轴,即轴向拼接\n",
    "    #此时的y矩阵记录的情感的结果值，1表示正面，0表示负面\n",
    "    print(\"y size=\",y.shape)\n",
    "    #Sklearn-train_test_split随机划分训练集和测试集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos['words'], neg['words'])), y, test_size=0.2,  random_state=1)\n",
    "    #划分测试集和训练集。在这里严格意义说是验证集，并不是测试集。\n",
    "    \n",
    "    #random_state：是随机数的种子。\n",
    "    #随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。\n",
    "    #比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。\n",
    "    #随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：\n",
    "    #种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。\n",
    "    \n",
    "    np.save('svm_data/y_train.npy',y_train)#先存储下训练集\n",
    "    np.save('svm_data/y_test.npy',y_test)\n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对每个句子的所有词向量取均值，来生成一个句子的vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sentence_vector(text, size,imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))#向量逐点求和\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算词向量\n",
    "def get_train_vecs(x_train,x_test):\n",
    "    n_dim = 300\n",
    "    #初始化模型和词表\n",
    "    imdb_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "    #其实参数很多，min_count表示对字典做截断。词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "    imdb_w2v.build_vocab(x_train)#收集单词及其词频来构建一个内部字典树结构\n",
    "    \n",
    "    #在评论训练集上建模(可能会花费几分钟)total_examples=model.corpus_count\n",
    "    imdb_w2v.train(x_train,total_examples=imdb_w2v.corpus_count, epochs=imdb_w2v.iter)#训练神经网络\n",
    "    #train函数的调用可能由于版本\n",
    "    \n",
    "    train_vecs = np.concatenate([build_sentence_vector(z, n_dim,imdb_w2v) for z in x_train])\n",
    "    #train_vecs = scale(train_vecs)\n",
    "    \n",
    "    np.save('svm_data/train_vecs.npy',train_vecs)\n",
    "    print(train_vecs.shape)\n",
    "    #在测试集上训练\n",
    "    imdb_w2v.train(x_test, total_examples=imdb_w2v.corpus_count, epochs=imdb_w2v.iter)\n",
    "    #对于测试集的训练，如果只给定测试数据集，而不指定参数，会报错如下：\n",
    "    #ValueError: You must specify either total_examples or total_words, for proper alpha and progress calculations. \n",
    "    #The usual value is total_examples=model.corpus_count.\n",
    "    imdb_w2v.save('svm_data/w2v_model/w2v_model.pkl')\n",
    "    #Build test tweet vectors then scale\n",
    "    test_vecs = np.concatenate([build_sentence_vector(z, n_dim,imdb_w2v) for z in x_test])\n",
    "    #test_vecs = scale(test_vecs)\n",
    "    np.save('svm_data/test_vecs.npy',test_vecs)\n",
    "    print(test_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y size= (21105,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test = load_file_and_preprocessing()\n",
    "\n",
    "#train_vecs.npy\n",
    "#w2v_model.pkl\n",
    "#test_vecs.npy都进行保存 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files (x86)\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16884, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:supplied example count (21105) did not equal expected count (84420)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4221, 300)\n"
     ]
    }
   ],
   "source": [
    "get_train_vecs(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_vecs=np.load('svm_data/train_vecs.npy')\n",
    "    y_train=np.load('svm_data/y_train.npy')\n",
    "    test_vecs=np.load('svm_data/test_vecs.npy')\n",
    "    y_test=np.load('svm_data/y_test.npy') \n",
    "    return train_vecs,y_train,test_vecs,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs,y_train,test_vecs,y_test = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练svm模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_train(train_vecs,y_train,test_vecs,y_test):\n",
    "    clf=SVC(kernel='rbf',verbose=True)\n",
    "    clf.fit(train_vecs,y_train)\n",
    "    joblib.dump(clf, 'svm_data/svm_model/model.pkl')\n",
    "    print(clf.score(test_vecs,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.809049988154\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "svm_train(train_vecs,y_train,test_vecs,y_test)#稍微耗时,5分钟左右\n",
    "toc = time.time()\n",
    "print(\"cost time=\",toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建待预测句子的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predict_vecs(words):\n",
    "    n_dim = 300\n",
    "    imdb_w2v = Word2Vec.load('svm_data/w2v_model/w2v_model.pkl')\n",
    "    #imdb_w2v.train(words)\n",
    "    train_vecs = build_sentence_vector(words, n_dim,imdb_w2v)\n",
    "    #print train_vecs.shape\n",
    "    return train_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对单个句子进行情感判断 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_predict(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words_vecs=get_predict_vecs(words)#对输入句子转为向量\n",
    "    clf=joblib.load('svm_data/svm_model/model.pkl')#加载训练好的情感分析模型\n",
    "     \n",
    "    result=clf.predict(words_vecs)#对输入句子进行情感预测\n",
    "    \n",
    "    if int(result[0])==1:\n",
    "        print(string,' positive')\n",
    "    else:\n",
    "        print(string,' negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从3米高的地方摔下去就坏了  negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files (x86)\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "##对输入句子情感进行判断\n",
    "#string='电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "#string='牛逼的手机，从3米高的地方摔下去都没坏，质量非常好'    \n",
    "#string='从3米高的地方摔下去都没坏，还能够替我挡子弹啊'    正确\n",
    "#string='从3米高的地方摔下去就坏了'\n",
    "string='能够挡子弹'\n",
    "svm_predict(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
